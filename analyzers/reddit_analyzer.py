"""Analyseur de sentiment Reddit avec support CSV et APIs (Reddit + Pushshift)"""

import aiohttp
import asyncio
import numpy as np
import csv
import json
from datetime import datetime, timedelta
from textblob import TextBlob
from typing import Dict, List, Tuple
import logging

logger = logging.getLogger('TradingBot')


class RedditSentimentAnalyzer:
    """
    Analyse le sentiment des discussions Reddit pour chaque action
    Utilise l'API REST de Reddit (pas besoin de PRAW)
    """

    def __init__(self, csv_file: str = None, data_dir: str = 'data'):
        self.session = None
        self.sentiment_cache = {}  # Cache pour √©viter appels r√©p√©t√©s
        self.csv_file = csv_file
        self.csv_data = None  # Donn√©es charg√©es depuis le CSV
        self.data_dir = data_dir  # Dossier contenant les CSV par action
        self.csv_data_by_symbol = {}  # Cache des CSV charg√©s par ticker

        # Configuration des subreddits par ticker
        self.ticker_subreddits = {
            'NVDA': ['NVDA_Stock', 'stocks'],
            'AAPL': ['AAPL', 'stocks'],
            'GOOGL': ['GOOG_Stock', 'stocks'],
            'GOOG': ['GOOG_Stock', 'stocks'],
            'AMZN': ['amzn', 'stocks'],
            'META': ['stocks'],  # Recherche sur r/stocks avec $meta
            'TSLA': ['TSLA', 'stocks'],
            'BRK-B': ['BerkshireHathaway', 'stocks'],
            'JPM': ['JPMorganChase', 'stocks'],
            'V': ['stocks'],  # Recherche avec $visa
            'JNJ': ['ValueInvesting', 'stocks'],
            'WMT': ['stocks'],
            'MSFT': ['stocks'],
            'MA': ['stocks'],
            'PG': ['stocks'],
            'DIS': ['stocks'],
            'NFLX': ['stocks'],
            'ADBE': ['stocks'],
            'CRM': ['stocks'],
            'AMD': ['AMD_Stock', 'stocks'],
            'ORCL': ['stocks'],
            'INTC': ['intel', 'stocks'],
            'CSCO': ['stocks'],
            'PEP': ['stocks'],
            'COST': ['stocks'],
            'AVGO': ['stocks']
        }

        # Tickers qui n√©cessitent une recherche sp√©ciale
        self.special_search_tickers = {
            'META': 'meta',
            'V': 'visa',
            'JNJ': 'JNJ',
            'WMT': 'wmt',
            'BRK-B': 'berkshire'
        }

    async def get_session(self):
        if not self.session:
            # User-Agent requis par Reddit
            headers = {
                'User-Agent': 'TradingBot/1.0 (by /u/TradingBotUser)'
            }
            self.session = aiohttp.ClientSession(headers=headers)
        return self.session

    def load_csv_data(self, symbol: str = None):
        """Charge les donn√©es Reddit depuis le CSV

        Args:
            symbol: Si fourni, charge data/Sentiment_[SYMBOL].csv au lieu du csv_file global
        """
        try:
            import pandas as pd
            from pathlib import Path

            # D√©terminer quel fichier charger
            if symbol:
                # Charger le fichier sp√©cifique √† l'action
                csv_path = Path(self.data_dir) / f"Sentiment_{symbol}.csv"

                # V√©rifier si d√©j√† en cache
                if symbol in self.csv_data_by_symbol:
                    logger.debug(f"[Reddit] ‚úÖ Utilisation cache pour {symbol}")
                    return True

                if not csv_path.exists():
                    logger.warning(f"[Reddit] ‚ö†Ô∏è Fichier {csv_path} introuvable")
                    return False

                # Charger et mettre en cache
                data = pd.read_csv(csv_path)
                data['created'] = pd.to_datetime(data['created'])
                self.csv_data_by_symbol[symbol] = data
                logger.info(f"[Reddit] ‚úÖ {len(data)} posts charg√©s pour {symbol} depuis {csv_path}")
                return True
            else:
                # Charger le fichier global (ancien comportement)
                if self.csv_file is None:
                    logger.warning("[Reddit] Pas de fichier CSV sp√©cifi√©")
                    return False

                self.csv_data = pd.read_csv(self.csv_file)
                self.csv_data['created'] = pd.to_datetime(self.csv_data['created'])
                logger.info(f"[Reddit] ‚úÖ {len(self.csv_data)} posts charg√©s depuis {self.csv_file}")
                return True

        except Exception as e:
            logger.error(f"[Reddit] ‚ùå Erreur chargement CSV: {e}")
            return False

    def get_posts_from_csv(self, symbol: str, target_date: datetime, lookback_hours: int = 48) -> List[Dict]:
        """R√©cup√®re les posts Reddit depuis le CSV pour une date donn√©e"""
        # Essayer de charger le CSV sp√©cifique √† l'action
        csv_data = None

        if symbol in self.csv_data_by_symbol:
            csv_data = self.csv_data_by_symbol[symbol]
        else:
            # Tenter de charger le fichier sp√©cifique
            if not self.load_csv_data(symbol=symbol):
                # Fallback sur le CSV global si disponible
                if self.csv_data is None:
                    if not self.load_csv_data():
                        return []
                csv_data = self.csv_data
            else:
                csv_data = self.csv_data_by_symbol[symbol]

        try:
            # Normaliser la date
            if hasattr(target_date, 'tz') and target_date.tz is not None:
                target_date = target_date.replace(tzinfo=None)

            cutoff_time = target_date - timedelta(hours=lookback_hours)

            # Filtrer les posts par date
            mask = (csv_data['created'] >= cutoff_time) & (csv_data['created'] <= target_date)
            filtered_posts = csv_data[mask]

            # Convertir en liste de dictionnaires
            posts = []
            for _, row in filtered_posts.iterrows():
                posts.append({
                    'title': row.get('title', ''),
                    'body': row.get('body', ''),
                    'score': row.get('upvotes', 0) - row.get('downvotes', 0),  # Recalculer score
                    'upvotes': row.get('upvotes', 0),
                    'downvotes': row.get('downvotes', 0),
                    'created': row['created'],
                    'author': row.get('author', ''),
                    'source': row.get('source', '')
                })

            logger.debug(f"[Reddit CSV] {symbol}: {len(posts)} posts trouv√©s pour {target_date.strftime('%Y-%m-%d')}")
            return posts

        except Exception as e:
            logger.error(f"[Reddit CSV] Erreur filtrage: {e}")
            return []

    async def get_reddit_sentiment(self, symbol: str, target_date: datetime = None,
                                   lookback_hours: int = 168, save_csv: bool = False) -> Tuple[float, int, List[str], List[Dict]]:
        """
        R√©cup√®re et analyse le sentiment Reddit pour un ticker (OPTIMIS√â POUR LIVE TRADING)

        Args:
            symbol: Ticker de l'action
            target_date: Date cible (None = maintenant)
            lookback_hours: Fen√™tre de temps (d√©faut: 168h = 7 jours)
            save_csv: Si True, sauvegarde tous les posts en CSV

        Returns:
            Tuple[sentiment_score (0-100), post_count, sample_posts, all_posts_details]
        """
        try:
            # Normaliser la date
            if target_date is None:
                target_date = datetime.now()
            if hasattr(target_date, 'tz') and target_date.tz is not None:
                target_date = target_date.replace(tzinfo=None)

            # V√©rifier le cache
            cache_key = f"{symbol}_{target_date.strftime('%Y-%m-%d')}"
            if cache_key in self.sentiment_cache:
                return self.sentiment_cache[cache_key]

            all_posts = []
            session = await self.get_session()

            # LIVE TRADING: Utiliser UNIQUEMENT l'API Reddit (7 derniers jours)
            # Pas de CSV, pas de Pushshift ‚Üí rapide et r√©cent
            logger.info(f"[Reddit] {symbol}: R√©cup√©ration via API Reddit (7 derniers jours)")

            # R√©cup√©rer les subreddits configur√©s pour ce ticker
            subreddits = self.ticker_subreddits.get(symbol, ['stocks'])
            logger.info(f"[Reddit] {symbol}: Subreddits cibl√©s: {subreddits}")

            for subreddit in subreddits:
                # Utiliser API REST Reddit pour donn√©es r√©centes
                if subreddit == 'stocks' or symbol in self.special_search_tickers:
                    search_term = self.special_search_tickers.get(symbol, symbol)
                    posts = await self._search_reddit_comments(
                        session, subreddit, search_term, target_date, lookback_hours
                    )
                    logger.info(f"[Reddit] {symbol}: r/{subreddit} (recherche '{search_term}'): {len(posts)} posts")
                else:
                    posts = await self._get_subreddit_posts(
                        session, subreddit, target_date, lookback_hours
                    )
                    logger.info(f"[Reddit] {symbol}: r/{subreddit}: {len(posts)} posts")

                all_posts.extend(posts)

                # D√©lai r√©duit pour rate limiting
                await asyncio.sleep(0.5)

            # Analyser le sentiment
            if not all_posts:
                result = (0.0, 0, [], [])  # Score 0 si pas de posts (personne n'en parle)
                self.sentiment_cache[cache_key] = result
                logger.info(f"[Reddit] {symbol}: ‚ö†Ô∏è Score 0/100 (aucun post r√©cup√©r√©)")
                return result

            sentiments = []
            sample_posts = []

            for post in all_posts[:50]:  # Limiter √† 50 posts max
                text = post.get('title', '') + ' ' + post.get('body', '')
                if len(text) > 10:
                    blob = TextBlob(text)
                    sentiment = blob.sentiment.polarity  # -1 √† +1

                    # Pond√©rer par score (upvotes)
                    score = post.get('score', 1)
                    weight = min(score / 10, 3)  # Max 3x weight
                    weighted_sentiment = sentiment * weight

                    sentiments.append(weighted_sentiment)

                    # Garder quelques exemples
                    if len(sample_posts) < 5:
                        emoji = "üü¢" if sentiment > 0.1 else "üî¥" if sentiment < -0.1 else "üü°"
                        sample_posts.append(f"{emoji} {text[:100]}... (sentiment: {sentiment:.2f})")

            # Calculer le score moyen
            avg_sentiment = np.mean(sentiments) if sentiments else 0

            # Convertir en score 0-100
            # -1 (tr√®s n√©gatif) -> 0
            #  0 (neutre) -> 50
            # +1 (tr√®s positif) -> 100
            sentiment_score = (avg_sentiment + 1) * 50
            sentiment_score = max(0, min(100, sentiment_score))

            result = (sentiment_score, len(all_posts), sample_posts, all_posts)
            self.sentiment_cache[cache_key] = result

            logger.info(f"[Reddit] {symbol}: ‚úÖ Score {sentiment_score:.0f}/100 ({len(all_posts)} posts analys√©s)")

            return result

        except Exception as e:
            logger.debug(f"Erreur Reddit sentiment {symbol}: {e}")
            return 0.0, 0, [], []  # Score 0 en cas d'erreur (pas de donn√©es valides)

    async def _get_subreddit_posts(self, session: aiohttp.ClientSession, subreddit: str,
                                   target_date: datetime, lookback_hours: int) -> List[Dict]:
        """R√©cup√®re les posts r√©cents d'un subreddit"""
        try:
            url = f"https://www.reddit.com/r/{subreddit}/new.json"
            params = {'limit': 100}

            async with session.get(url, params=params, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    children = data.get('data', {}).get('children', [])
                    logger.info(f"[Reddit] r/{subreddit}: {len(children)} posts bruts r√©cup√©r√©s")

                    posts = []
                    cutoff_time = target_date - timedelta(hours=lookback_hours)
                    # Permettre une marge de 1 heure dans le futur pour les d√©calages d'horloge
                    future_margin = target_date + timedelta(hours=1)

                    for post in children:
                        post_data = post.get('data', {})
                        created_utc = post_data.get('created_utc', 0)
                        post_date = datetime.fromtimestamp(created_utc)

                        if cutoff_time <= post_date <= future_margin:
                            # Calculer upvotes/downvotes √† partir de score et upvote_ratio
                            score = post_data.get('score', 0)
                            upvote_ratio = post_data.get('upvote_ratio', 0.5)

                            if upvote_ratio > 0 and upvote_ratio != 0.5:
                                upvotes = int(score / (2 * upvote_ratio - 1))
                                downvotes = upvotes - score
                            else:
                                upvotes = max(0, score)
                                downvotes = 0

                            posts.append({
                                'title': post_data.get('title', ''),
                                'body': post_data.get('selftext', ''),
                                'score': score,  # Gard√© pour la logique de pond√©ration
                                'upvotes': upvotes,
                                'downvotes': downvotes,
                                'created': post_date
                            })

                    logger.info(f"[Reddit] r/{subreddit}: {len(posts)} posts gard√©s apr√®s filtrage temporel")
                    return posts
                else:
                    logger.warning(f"[Reddit] r/{subreddit}: Status {response.status}")

        except Exception as e:
            logger.warning(f"[Reddit] Erreur r√©cup√©ration r/{subreddit}: {e}")

        return []

    async def _search_reddit_comments(self, session: aiohttp.ClientSession, subreddit: str,
                                     search_term: str, target_date: datetime,
                                     lookback_hours: int) -> List[Dict]:
        """Recherche des commentaires sur r/stocks avec le ticker"""
        try:
            # Recherche avec le ticker
            url = f"https://www.reddit.com/r/{subreddit}/search.json"
            params = {
                'q': f'${search_term}' if subreddit == 'stocks' else search_term,
                'restrict_sr': 'on',
                'sort': 'new',
                'limit': 100,
                't': 'week'  # Derni√®re semaine
            }

            async with session.get(url, params=params, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    children = data.get('data', {}).get('children', [])
                    logger.info(f"[Reddit] r/{subreddit} (recherche '{search_term}'): {len(children)} posts bruts r√©cup√©r√©s")

                    posts = []
                    cutoff_time = target_date - timedelta(hours=lookback_hours)
                    # Permettre une marge de 1 heure dans le futur pour les d√©calages d'horloge
                    future_margin = target_date + timedelta(hours=1)

                    for post in children:
                        post_data = post.get('data', {})
                        created_utc = post_data.get('created_utc', 0)
                        post_date = datetime.fromtimestamp(created_utc)

                        if cutoff_time <= post_date <= future_margin:
                            # Calculer upvotes/downvotes √† partir de score et upvote_ratio
                            score = post_data.get('score', 0)
                            upvote_ratio = post_data.get('upvote_ratio', 0.5)

                            if upvote_ratio > 0 and upvote_ratio != 0.5:
                                upvotes = int(score / (2 * upvote_ratio - 1))
                                downvotes = upvotes - score
                            else:
                                upvotes = max(0, score)
                                downvotes = 0

                            posts.append({
                                'title': post_data.get('title', ''),
                                'body': post_data.get('selftext', ''),
                                'score': score,  # Gard√© pour la logique de pond√©ration
                                'upvotes': upvotes,
                                'downvotes': downvotes,
                                'created': post_date
                            })

                    logger.info(f"[Reddit] r/{subreddit} (recherche '{search_term}'): {len(posts)} posts gard√©s apr√®s filtrage")
                    return posts
                else:
                    logger.warning(f"[Reddit] r/{subreddit} (recherche '{search_term}'): Status {response.status}")

        except Exception as e:
            logger.warning(f"[Reddit] Erreur recherche '{search_term}' sur r/{subreddit}: {e}")

        return []

    async def _get_pushshift_posts(self, session: aiohttp.ClientSession, subreddit: str,
                                   target_date: datetime, lookback_hours: int) -> List[Dict]:
        """R√©cup√®re TOUS les posts historiques via PullPush/Pushshift avec pagination ILLIMIT√âE"""
        try:
            logger.info(f"   [Pushshift] R√©cup√©ration COMPL√àTE r/{subreddit} - TOUS LES POSTS")

            url = "https://api.pullpush.io/reddit/search/submission"
            all_posts = []
            before = int(target_date.timestamp())
            iteration = 0

            # Boucle INFINIE jusqu'√† √©puisement des posts
            while True:
                iteration += 1
                params = {
                    'subreddit': subreddit,
                    'before': before,
                    'size': 500,  # Max par requ√™te
                    'sort': 'desc',
                    'sort_type': 'created_utc'
                }

                async with session.get(url, params=params, timeout=20) as response:
                    if response.status == 200:
                        text = await response.text()
                        data = json.loads(text)
                        data_posts = data.get('data', [])

                        if not data_posts or len(data_posts) == 0:
                            logger.info(f"   [Pushshift] üèÅ Fin de pagination (plus de posts)")
                            break

                        # Ajouter tous les posts
                        for post in data_posts:
                            try:
                                created_utc = post.get('created_utc', 0)
                                post_date = datetime.fromtimestamp(created_utc)

                                # Calculer upvotes/downvotes (Pushshift ne fournit pas toujours upvote_ratio)
                                score = post.get('score', 0)
                                upvote_ratio = post.get('upvote_ratio', None)

                                if upvote_ratio is not None and upvote_ratio > 0 and upvote_ratio != 0.5:
                                    upvotes = int(score / (2 * upvote_ratio - 1))
                                    downvotes = upvotes - score
                                else:
                                    upvotes = max(0, score)
                                    downvotes = 0

                                all_posts.append({
                                    'title': post.get('title', ''),
                                    'body': post.get('selftext', ''),
                                    'score': score,
                                    'upvotes': upvotes,
                                    'downvotes': downvotes,
                                    'created': post_date
                                })
                            except Exception:
                                continue

                        # Mettre √† jour 'before' pour la prochaine page
                        last_post_time = data_posts[-1].get('created_utc', 0)

                        # V√©rifier qu'on avance (√©viter boucle infinie)
                        if last_post_time >= before:
                            logger.warning(f"   [Pushshift] ‚ö†Ô∏è Timestamp ne diminue pas, arr√™t")
                            break

                        before = last_post_time
                        logger.info(f"   [Pushshift] Page {iteration}: +{len(data_posts)} posts | Total: {len(all_posts)}")

                        # D√©lai pour rate limiting
                        await asyncio.sleep(1.5)
                    else:
                        logger.error(f"   [Pushshift] Status {response.status}, arr√™t")
                        break

            # Trier par date
            all_posts.sort(key=lambda x: x['created'], reverse=True)

            logger.info(f"   [Pushshift] ‚úÖ {len(all_posts)} posts TOTAUX r√©cup√©r√©s pour r/{subreddit}")
            return all_posts

        except Exception as e:
            logger.error(f"   [Pushshift] ‚ùå Erreur subreddit {subreddit}: {e}")
            import traceback
            traceback.print_exc()

        return []

    async def _search_pushshift(self, session: aiohttp.ClientSession, subreddit: str,
                                search_term: str, target_date: datetime,
                                lookback_hours: int) -> List[Dict]:
        """Recherche TOUS les posts historiques avec un terme via Pushshift - pagination ILLIMIT√âE"""
        try:
            query = f'${search_term}' if subreddit == 'stocks' else search_term
            logger.info(f"   [Pushshift] R√©cup√©ration COMPL√àTE '{query}' r/{subreddit} - TOUS LES POSTS")

            url = "https://api.pullpush.io/reddit/search/submission"
            all_posts = []
            before = int(target_date.timestamp())
            iteration = 0

            # Boucle INFINIE jusqu'√† √©puisement des posts
            while True:
                iteration += 1
                params = {
                    'subreddit': subreddit,
                    'q': query,
                    'before': before,
                    'size': 500,
                    'sort': 'desc',
                    'sort_type': 'created_utc'
                }

                async with session.get(url, params=params, timeout=20) as response:
                    if response.status == 200:
                        text = await response.text()
                        data = json.loads(text)
                        data_posts = data.get('data', [])

                        if not data_posts or len(data_posts) == 0:
                            logger.info(f"   [Pushshift] üèÅ Fin de pagination pour '{query}'")
                            break

                        # Ajouter tous les posts
                        for post in data_posts:
                            try:
                                created_utc = post.get('created_utc', 0)
                                post_date = datetime.fromtimestamp(created_utc)

                                # Calculer upvotes/downvotes
                                score = post.get('score', 0)
                                upvote_ratio = post.get('upvote_ratio', None)

                                if upvote_ratio is not None and upvote_ratio > 0 and upvote_ratio != 0.5:
                                    upvotes = int(score / (2 * upvote_ratio - 1))
                                    downvotes = upvotes - score
                                else:
                                    upvotes = max(0, score)
                                    downvotes = 0

                                all_posts.append({
                                    'title': post.get('title', ''),
                                    'body': post.get('selftext', ''),
                                    'score': score,
                                    'upvotes': upvotes,
                                    'downvotes': downvotes,
                                    'created': post_date
                                })
                            except Exception:
                                continue

                        # Mettre √† jour 'before' pour la prochaine page
                        last_post_time = data_posts[-1].get('created_utc', 0)

                        # V√©rifier qu'on avance
                        if last_post_time >= before:
                            logger.warning(f"   [Pushshift] ‚ö†Ô∏è Timestamp ne diminue pas, arr√™t")
                            break

                        before = last_post_time
                        logger.info(f"   [Pushshift] Page {iteration}: +{len(data_posts)} posts | Total: {len(all_posts)}")

                        # D√©lai pour rate limiting
                        await asyncio.sleep(1.5)
                    else:
                        logger.error(f"   [Pushshift] Status {response.status}, arr√™t")
                        break

            # Trier par date
            all_posts.sort(key=lambda x: x['created'], reverse=True)

            logger.info(f"   [Pushshift] ‚úÖ {len(all_posts)} posts TOTAUX r√©cup√©r√©s pour '{query}'")
            return all_posts

        except Exception as e:
            logger.error(f"   [Pushshift] ‚ùå Erreur search {search_term}: {e}")
            import traceback
            traceback.print_exc()

        return []

    def save_posts_to_csv(self, symbol: str, posts: List[Dict], source: str):
        """Sauvegarde les posts dans un fichier CSV"""
        try:
            filename = f"reddit_posts_{symbol}_{source}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=['created', 'title', 'body', 'upvotes', 'downvotes'])
                writer.writeheader()

                for post in posts:
                    writer.writerow({
                        'created': post['created'].strftime('%Y-%m-%d %H:%M:%S'),
                        'title': post['title'],
                        'body': post['body'],
                        'upvotes': post.get('upvotes', 0),
                        'downvotes': post.get('downvotes', 0)
                    })

            logger.info(f"   [CSV] ‚úÖ {len(posts)} posts sauvegard√©s dans {filename}")
            return filename
        except Exception as e:
            logger.error(f"   [CSV] ‚ùå Erreur sauvegarde: {e}")
            return None

    async def close(self):
        if self.session:
            await self.session.close()
