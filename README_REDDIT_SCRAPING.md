# Guide de Scraping Reddit pour le Trading Bot

## ğŸ“ Structure des fichiers

```
Trading_Bot/
â”œâ”€â”€ data/                          # Dossier de donnÃ©es
â”‚   â”œâ”€â”€ Sentiment_NVDA.csv        # DonnÃ©es sentiment pour NVDA
â”‚   â”œâ”€â”€ Sentiment_AAPL.csv        # DonnÃ©es sentiment pour AAPL
â”‚   â””â”€â”€ Sentiment_[TICKER].csv    # Etc...
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scrape_all_stocks.py      # Script principal pour scraper toutes les actions
â”‚   â””â”€â”€ scrape_pushshift_csv.py   # Ancien script (uniquement NVDA)
â”‚
â”œâ”€â”€ analyzers/
â”‚   â””â”€â”€ reddit_analyzer.py         # Analyseur de sentiment (lit automatiquement les CSV)
â”‚
â””â”€â”€ config_stocks.py               # Configuration centralisÃ©e des actions
```

## ğŸš€ Comment scraper les donnÃ©es Reddit

### 1. Scraper toutes les actions configurÃ©es

```bash
cd scripts/
python scrape_all_stocks.py
```

Ce script va :
- Scraper toutes les actions dÃ©finies dans `config_stocks.py`
- RÃ©cupÃ©rer les posts via Reddit API (posts rÃ©cents) et Pushshift (historique)
- Sauvegarder chaque action dans `data/Sentiment_[TICKER].csv`

### 2. Ajouter une nouvelle action

Ã‰ditez le fichier `config_stocks.py` :

```python
STOCK_CONFIGS = {
    'MSFT': {  # Nouveau ticker
        'sources': [
            # Option 1 : Subreddit dÃ©diÃ©
            {'type': 'subreddit', 'name': 'microsoft'},

            # Option 2 : Recherche dans r/stocks
            {'type': 'search', 'subreddit': 'stocks', 'query': '$MSFT'}
        ]
    }
}
```

**RÃ¨gles** :
- Si l'action a un subreddit dÃ©diÃ© (ex: r/NVDA_Stock), utilisez `type: 'subreddit'`
- Sinon, utilisez `type: 'search'` avec r/stocks et le ticker
- Vous pouvez combiner plusieurs sources (subreddit dÃ©diÃ© + recherche)

### 3. Format des fichiers CSV

Chaque fichier `data/Sentiment_[TICKER].csv` contient :

```csv
created,source,title,body,upvotes,downvotes,author,url,id
2025-11-06 15:30:00,r/NVDA_Stock,"NVDA earnings beat!","Great quarter...",145,12,user123,https://...,abc123
```

Colonnes :
- **created** : Date/heure du post
- **source** : Subreddit source (ex: r/NVDA_Stock)
- **title** : Titre du post
- **body** : Contenu du post
- **upvotes** : Nombre d'upvotes
- **downvotes** : Nombre de downvotes
- **author** : Auteur du post
- **url** : URL du post
- **id** : ID unique du post

## ğŸ“Š Utilisation dans le backtest

Le `RedditSentimentAnalyzer` charge automatiquement le bon fichier CSV selon l'action :

```python
from analyzers.reddit_analyzer import RedditSentimentAnalyzer

# Initialiser l'analyseur (va chercher dans data/)
analyzer = RedditSentimentAnalyzer(data_dir='data')

# Analyser le sentiment pour NVDA
sentiment, post_count, samples, posts = await analyzer.get_reddit_sentiment(
    symbol='NVDA',
    target_date=datetime.now(),
    lookback_hours=48
)

print(f"Sentiment NVDA: {sentiment}/100 ({post_count} posts)")
```

**Le systÃ¨me charge automatiquement** `data/Sentiment_NVDA.csv` !

## ğŸ¯ Actions configurÃ©es actuellement

| Ticker | Subreddits / Sources |
|--------|---------------------|
| NVDA   | r/NVDA_Stock + r/stocks ($NVDA) |
| AAPL   | r/AAPL + r/stocks ($AAPL) |
| GOOG   | r/GOOG_Stock + r/stocks ($GOOG) |
| AMZN   | r/amzn + r/stocks ($AMZN) |
| META   | r/stocks ($meta) |
| TSLA   | r/TSLA + r/stocks ($TSLA) |
| BRK.B  | r/BerkshireHathaway + r/stocks ($BRK) |
| JPM    | r/JPMorganChase + r/stocks ($JPM) |
| V      | r/stocks ($visa) |
| JNJ    | r/ValueInvesting (JNJ) + r/stocks ($JNJ) |
| WMT    | r/stocks (wmt) |

## âš™ï¸ Configuration avancÃ©e

### Scraper une seule action

Modifiez `scrape_all_stocks.py` pour limiter le scraping :

```python
# Scraper uniquement NVDA et AAPL
STOCK_CONFIGS = {k: v for k, v in STOCK_CONFIGS.items() if k in ['NVDA', 'AAPL']}
```

### Modifier les paramÃ¨tres de scraping

Dans `scrape_all_stocks.py`, vous pouvez ajuster :
- `limit` : Nombre max de posts par source (dÃ©faut: 1000)
- `size` : Taille des pages Pushshift (dÃ©faut: 100)
- DÃ©lais entre requÃªtes pour Ã©viter rate limiting

## ğŸ”§ DÃ©pannage

### Erreur "Fichier Sentiment_XXX.csv introuvable"

1. VÃ©rifiez que vous avez bien scrapÃ© l'action avec `scrape_all_stocks.py`
2. VÃ©rifiez que le fichier existe dans `data/`

### Rate limiting Reddit

Si vous Ãªtes bloquÃ© par Reddit :
- Augmentez les dÃ©lais entre requÃªtes (`await asyncio.sleep(...)`)
- Utilisez un VPN ou changez d'IP
- Attendez quelques heures

### Pushshift ne rÃ©pond pas

Pushshift/PullPush peut Ãªtre instable :
- Le script a des retries automatiques (3 tentatives)
- Si Ã§a Ã©choue, les donnÃ©es Reddit API seront quand mÃªme sauvegardÃ©es
- RÃ©essayez plus tard pour l'historique Pushshift

## ğŸ“ Notes importantes

1. **Reddit API** : LimitÃ© aux ~1000 posts les plus rÃ©cents par source
2. **Pushshift** : AccÃ¨s Ã  l'historique complet mais peut Ãªtre lent/instable
3. **DÃ©duplication** : Les doublons entre sources sont automatiquement supprimÃ©s
4. **Rate limiting** : Respectez les limites pour Ã©viter d'Ãªtre bloquÃ©

## ğŸ“ Exemples d'utilisation

### Exemple 1 : Scraper toutes les actions

```bash
python scripts/scrape_all_stocks.py
```

### Exemple 2 : Analyser le sentiment dans le backtest

```python
from backtest.backtest_engine import RealisticBacktestEngine

# Le backtest va automatiquement charger data/Sentiment_[SYMBOL].csv
engine = RealisticBacktestEngine()
results = await engine.backtest_with_news_validation('NVDA', months=6)
```

### Exemple 3 : Ajouter MSFT

1. Ã‰ditez `config_stocks.py` :
```python
'MSFT': {
    'sources': [
        {'type': 'subreddit', 'name': 'microsoft'},
        {'type': 'search', 'subreddit': 'stocks', 'query': '$MSFT'}
    ]
}
```

2. Scrapez :
```bash
python scripts/scrape_all_stocks.py
```

3. Le fichier `data/Sentiment_MSFT.csv` sera crÃ©Ã© automatiquement

## ğŸ“ Support

Pour toute question ou problÃ¨me, consultez :
- `GUIDE_TEST.md` : Guide de test complet
- `README_STRUCTURE.md` : Structure du projet
- `RECAP_FINAL.md` : RÃ©capitulatif du systÃ¨me
